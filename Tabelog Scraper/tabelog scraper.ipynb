{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "We're going to Japan! One of the things that we're the most excited about for the trip is all of the great food places in Japan. We're certain that we'll be able to find good food almost anywhere but the sheer number of excellent restaurants in Japan is mindboggling. The most popular Japanese Restaurant rating site is Tabelog which is similar to Yelp. Unfortunately, their English version of the website does not allow you to search for the restaurants by name and you can only use filters to find a place. In order to make things easier to view, I thought it would be good to scrape the areas I was interested in for different uses. \n",
    "\n",
    "To help us orient ourselves, I wanted to have a map showing the top rated spots in the cities so we could keep an eye out while we wander. The goal of this is to put everything onto one spreadsheet and Google Maps so that I can have the flexibility of finding spots on the go or to filter out what I want to eat from the top rated restaurants. The current workflow is scraping tabelog.com for the ranking data $\\rightarrow$ outputtings a .csv file $\\rightarrow$ importing to Google Sheets $\\rightarrow$ export the addresses to Google MyMaps. This will import everything onto a layer that can be viewed when using Google Maps and give us the ability to do some filtering and searching in Google Sheets while on the go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code\n",
    "I will be using BeautifulSoup to scrape the webpages and pandas to organize and export the data. I'm able to set the amount of pages that I want to grab from the top rated categories as well as the region I'm interested in. If I wanted to, I could loop this process over a list of cities. However, this is not possible in reality as I would get rate limited and kicked off Tabelog for accessing too many pages. Thus, I utilized single runs for each city. It's possible that I could add in a delay after every page to prevent being kicked off if I wanted to try automating this for a list of regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries to use\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Set number of pages to get\n",
    "max_pages = 0\n",
    "page_count = 1\n",
    "city = '' # city name like 'kyoto'. blank for not running when testing other bits\n",
    "\n",
    "names = []\n",
    "ratings = []\n",
    "reviews = []\n",
    "areas = []\n",
    "genres = []\n",
    "urls = []\n",
    "lunch_price = []\n",
    "dinner_price = []\n",
    "addresses = []\n",
    "\n",
    "# Base information\n",
    "url = 'https://tabelog.com/en/' + city + '/rstLst/' + str(page_count) +'/?SrtT=rt'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text,'html.parser')\n",
    "\n",
    "while page_count <= max_pages:\n",
    "\n",
    "    print('On page ', page_count)\n",
    "    # Extracting names, ratings, and areas from the soup object\n",
    "    names_list = soup.find_all(class_='list-rst__rst-name-target cpy-rst-name')\n",
    "    ratings_list = soup.find_all(class_='c-rating__val c-rating__val--strong list-rst__rating-val')\n",
    "    reviews_list = soup.find_all(class_='list-rst__rvw-count-num cpy-review-count')\n",
    "    area_list = soup.find_all(class_=\"list-rst__area-genre cpy-area-genre\")\n",
    "    soup_list = soup.find_all('div', class_='list-rst js-bookmark js-rst-cassette-wrap')\n",
    "\n",
    "    # Extracting data and populating lists\n",
    "    for i, (name, rating, area, review) in enumerate(zip(names_list, ratings_list, area_list, reviews_list)):\n",
    "        names.append(name.get_text(strip=True))\n",
    "        ratings.append(rating.get_text(strip=True))\n",
    "        reviews.append(review.get_text(strip=True))\n",
    "\n",
    "        # Extracting the URL for each restaurant on the page\n",
    "        urls.append(soup_list[i].get('data-detail-url'))\n",
    "        \n",
    "        # Getting URL information for the address\n",
    "        temp_page = requests.get(urls[-1])\n",
    "        url_soup = BeautifulSoup(temp_page.text,'html.parser')\n",
    "        addresses.append(url_soup.find(class_=\"rstinfo-table__address\").get_text(strip=True)) \n",
    "\n",
    "        # Extracting lunch and dinner prices\n",
    "        dinner_price.append(soup_list[i].find_all('span', class_='c-rating-v3__val')[0].get_text(strip=True))\n",
    "        lunch_price.append(soup_list[i].find_all('span', class_='c-rating-v3__val')[1].get_text(strip=True)) \n",
    "\n",
    "        # Extracting area and genre. Some places don't show a nearest station so there isnt a / symbol to separate.\n",
    "        if '/' in (soup_list[i].find('div', class_='list-rst__area-genre cpy-area-genre').get_text(strip=True)):\n",
    "            area_genre = soup_list[i].find('div', class_='list-rst__area-genre cpy-area-genre').get_text(strip=True)\n",
    "            temp_station = area_genre.split(' / ')[0].strip()\n",
    "            temp_station = temp_station[:-5]\n",
    "            areas.append(temp_station)\n",
    "            genres.append(area_genre.split(' / ')[1].strip())\n",
    "        else:\n",
    "            areas.append('-')\n",
    "            genres.append(soup_list[i].find('div', class_='list-rst__area-genre cpy-area-genre').get_text().replace('\\n', ''))\n",
    "\n",
    "    page_count += 1\n",
    "\n",
    "    # Get new page of results to scrape and reload\n",
    "    new_url = 'https://tabelog.com/en/'+city+'/rstLst/' + str(page_count) +'/?SrtT=rt'\n",
    "    page = requests.get(new_url)\n",
    "    soup = BeautifulSoup(page.text,'html.parser')\n",
    "\n",
    "# Creating a dictionary with the extracted data\n",
    "data = {\n",
    "    'Name': names,\n",
    "    'Rating': ratings,\n",
    "    'Reviews': reviews,\n",
    "    'Station': areas,\n",
    "    'Address': addresses,\n",
    "    'Genre': genres,\n",
    "    'Lunch Price': lunch_price,\n",
    "    'Dinner Price': dinner_price,\n",
    "    'URL': urls\n",
    "}\n",
    "\n",
    "# Creating a DataFrame from the data dictionary\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(city+'-top'+str(max_pages)+'pages.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it outputs, we get something like this if we import it to Google Sheets to view\n",
    "\n",
    "<div>\n",
    "<img src=\"img\\sheets_preview.png\" width=\"1024\"/>\n",
    "</div>\n",
    "\n",
    "Success! Now that it works, I run the same script for a couple regions that we're interested in visiting as well as a general Top X of Japan where X is whatever number I want. The data can be formatted to look better and used for the trip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use case: Finding what is close by - Importing data into Google Maps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the sheets can be prepared, I can now set up some Google Map layers to view the list of places I scraped while using Google Maps to navigate. In order to do this with the spreadsheet that I made, I need to use Google MyMaps instead of Google Maps. MyMaps is different than just Maps as it allows for the importing of data from external sources like .csvs, Google Sheets, etc. \n",
    "\n",
    "Once I've uploaded different spreadsheets that I made from scraping for the Top 1000 restaurants in Kyoto, Hokkaido, Tokyo, Osaka, and Japan onto MyMaps, I have something like this! Unfortunately, some of the addresses that were scraped were unable to be found when searching them up on Google Maps so they are not shown on the map\n",
    "<div>\n",
    "<img src=\"img\\mymaps.png\" width=\"512\"/>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use case: Finding a food you want - Linking Google Sheets to Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the maps integration is done, a little bit of work needs to be done to the data to make it usable under different situations. The current use case for the maps integration is to locate places and give us ideas when we are walking around specific areas looking for somewhere to eat. But what if we want to look for something specific like ice cream, ramen, or even curry? Filtering based on the genres generated from the scraped data should give us some the information.\n",
    "\n",
    "When looking at the spreadsheet, the genres that were obtained from Tabelog may sometimes contain multiple tags. \n",
    "<div>\n",
    "<img src=\"img\\genres.png\"/>\n",
    "</div>\n",
    "\n",
    "However, this is still searchable on mobile and on desktop versions of the sheets as we can type what we want to find, even if multiple tags are in the same cell. \n",
    "<div>\n",
    "<img src=\"img\\filter.png\"/>\n",
    "</div>\n",
    "There is some room for improvement here to make things a little more streamlined by making the tags in different cells since they are separated with commas. This would make it so that tags are searchable by clicking on the filter rather than typing something out. Alternatively, I could make separate columns for each tag associated with the restaurants and filter for them across the selected columns. However, these aren't convenient when using the spreadsheet on a mobile device so the current method will suffice.\n",
    "\n",
    "Now that I know what options are available for ice cream, I want to check out where it is. Luckily, Google Sheets have Smart Chips which allow a link that opens up that location on Google Maps. Unluckily, it only works with Google Maps links that are for a specific place or entering the address manually and selecting the correct option when creating the Smart Chip. Both of these can't be used without access to the developer APIs or by going through them individually unfortunately. However, a workaround that I thought about was creating a Google Maps link query based on the address that I scraped which can be clicked on to open up Google Maps. I could make it more specific as well by including the name as some locations have multiple listings. To create a link that is recognized by Sheets, I use the HYPERLINK and CONCATENATE functions on Google Sheets along with the url (\"https://www.google.com/maps/search/?api=1&query=\") to make the Google Maps search links found from the Google Maps API (https://developers.google.com/maps/documentation/urls/get-started). Since some of the restaurants have spaces in their names and this does not work super well when creating Google Maps links that are detectable by Google Sheets, a quick SUBSTITUTE function can do the job to change the white spaces to + signs which are accepted.\n",
    "\n",
    "Full function used was =HYPERLINK(CONCATENATE(\"https://www.google.com/maps/search/?api=1&query=\", SUBSTITUTE(B2,\" \",\"+\"),\"+\", SUBSTITUTE(F2,\" \",\"+\")), \"Link\")\n",
    "\n",
    "Testing it for our first entry gives us this:\n",
    "\n",
    "<div>\n",
    "<img src=\"img\\maps_link.png\"/>\n",
    "</div>\n",
    "\n",
    "Which lets us click on something from the Sheets view to bring us to the Maps location!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting for usability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can use the sheet to look for places and open them up on Google Maps, I want to edit the spreadsheet for better readability. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lunch and Dinner Price - Changing text to numbers and conditional formatting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs from the lunch and dinner prices for each restaurant were taken as text ranges like JPY 40,000 ~ JPY 49,999. I want to modify these cells to make them searchable and easy to read\n",
    "<div>\n",
    "<img src=\"img\\sheets_preview.png\" width=\"1024\"/>\n",
    "</div>\n",
    "\n",
    "Since the ranges given from Tabelog are all specific ranges, I can replace them with a substitute function and a multiple conditional using IFS() to check which range it is. As an example, I will say the range of JPY 40,000 ~ JPY 49,999 will be changed to the average of the two which is 45000 for a number that can be used for conditional formatting and sorting. In the case of the edge ranges of <999 JPY, >100,000 JPY, or '-' (did not exist in the scrape), numbers of 999 and 100,000, and a blank cell will be used respectively.\n",
    "\n",
    "\n",
    "An example would be =IFS(I2 = \"～JPY 999\", 999 ,I2 = \"JPY 1,000～JPY 1,999\", 1500 ,I2 = \"JPY 2,000～JPY 2,999\", 2500,....) and keep listing them for all ranges known. I apply these in a new column where I can then use conditional formatting with a colour scale to give me a visual indicator of their prices. The initial price columns with ranges have been moved to the far right so that the formatted price columns are closer to the names and locations\n",
    "\n",
    "<div>\n",
    "<img src=\"img\\conditional_price.png\" width=\"1024\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final touch ups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add in some extra formatting to help visibility with alternating row colours and reordered columns and it's complete for the Tokyo list! Repeat the process for other regions\n",
    "\n",
    "<div>\n",
    "<img src=\"img\\final.png\" width=\"1024\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important info for each restaurant nested in\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All info per restaurant\n",
    "```\n",
    "<div class=\"list-rst__rst-data\">\n",
    "```\n",
    "```\n",
    "<div class=\"list-rst js-bookmark js-rst-cassette-wrap\" data-detail-url=\"https://tabelog.com/en/tokyo/A1302/A130204/13018162/\" data-rst-id=\"13018162\">\n",
    "```\n",
    "\n",
    "Restaurant name\n",
    "```\n",
    "<a class=\"list-rst__rst-name-target cpy-rst-name\" target=\"_blank\" rel=\"noopener\" data-list-dest=\"item_top\" href=\"https://tabelog.com/en/tokyo/A1302/A130204/13018162/\">Sugi ta</a>\n",
    "```\n",
    "\n",
    "Prices\n",
    "```\n",
    "<li class=\"c-rating-v3 list-rst__info-item\">\n",
    "                            <i aria-label=\"Average dinner price\" class=\"c-rating-v3__time c-rating-v3__time--dinner\" role=\"img\"></i><span class=\"c-rating-v3__val\">JPY 40,000～JPY 49,999</span>\n",
    "                          </li>\n",
    "\n",
    "<li class=\"c-rating-v3 list-rst__info-item\">\n",
    "                            <i aria-label=\"Average lunch price\" class=\"c-rating-v3__time c-rating-v3__time--lunch\" role=\"img\"></i><span class=\"c-rating-v3__val\">JPY 40,000～JPY 49,999</span>\n",
    "                          </li>\n",
    "```\n",
    "Area\n",
    "```\n",
    "<div class=\"list-rst__area-genre cpy-area-genre\">\n",
    "                            Suitengumae Sta. / Sushi\n",
    "                          </div>\n",
    "```\n",
    "\n",
    "Rating\n",
    "```\n",
    "<p class=\"c-rating c-rating--xxl c-rating--val45 list-rst__rating-total is-highlight cpy-total-score\">\n",
    "                            <i class=\"c-rating__star list-rst__rating-star\"></i><span class=\"c-rating__val c-rating__val--strong list-rst__rating-val\">4.66</span>\n",
    "                          </p>\n",
    "```\n",
    "\n",
    "Rating Badge\n",
    "```\n",
    "<p class=\"c-rating-border c-rating-border--excellent\">\n",
    "                                        Excellent\n",
    "                                      </p>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set number of pages to get\n",
    "# max_pages = 1\n",
    "# page_count = 1\n",
    "\n",
    "# df = pd.DataFrame(columns = ['Name',\n",
    "#             'Rating',\n",
    "#             'Lunch Price',\n",
    "#             'Dinner Price',\n",
    "#             'Station',\n",
    "#             'Genre',\n",
    "#             'Link',\n",
    "#             'Address'])\n",
    "\n",
    "# while page_count <= max_pages:\n",
    "\n",
    "#     # Base information\n",
    "#     print('On page ', page_count)\n",
    "#     url = 'https://tabelog.com/en/tokyo/rstLst/' + str(page_count) +'/?SrtT=rt'\n",
    "#     page = requests.get(url)\n",
    "#     soup = BeautifulSoup(page.text,'html.parser')\n",
    "\n",
    "#     # Extracting names, ratings, and areas from the soup object\n",
    "#     names_list = soup.find_all(class_='list-rst__rst-name-target cpy-rst-name')\n",
    "#     ratings_list = soup.find_all(class_='c-rating__val c-rating__val--strong list-rst__rating-val')\n",
    "#     area_list = soup.find_all(class_=\"list-rst__area-genre cpy-area-genre\")\n",
    "#     soup_list = soup.find_all('div', class_='list-rst js-bookmark js-rst-cassette-wrap')\n",
    "\n",
    "#     # Creating a DataFrame from the data dictionary\n",
    "\n",
    "\n",
    "#     names = [0]*len(names_list)\n",
    "#     ratings = [0]*len(names_list)\n",
    "#     areas = [0]*len(names_list)\n",
    "#     genres = [0]*len(names_list)\n",
    "#     urls = [0]*len(names_list)\n",
    "#     address = [0]*len(names_list)\n",
    "#     lunch_price = [0]*len(names_list)\n",
    "#     dinner_price = [0]*len(names_list)\n",
    "\n",
    "#     # Creating a dictionary with the extracted data\n",
    "#     data = {\n",
    "#             'Name': names,\n",
    "#             'Rating': ratings,\n",
    "#             'Lunch Price': lunch_price,\n",
    "#             'Dinner Price': dinner_price,\n",
    "#             'Station': areas,\n",
    "#             'Genre': genres,\n",
    "#             'Link': urls,\n",
    "#             'Address': address\n",
    "#         }\n",
    "\n",
    "#     # Extracting lunch and dinner prices, areas, genre of food\n",
    "#     for i in range(len(soup_list)):\n",
    "#         soup_tmp = soup_list[i].find_all('span', class_='c-rating-v3__val')\n",
    "#         soup_tmp_area = soup_list[i].find('div', class_='list-rst__area-genre cpy-area-genre')\n",
    "\n",
    "#         names[i] = (names_list[i].text)\n",
    "#         ratings[i] = (ratings_list[i].text)\n",
    "\n",
    "#         dinner_price[i] = (soup_tmp[0].text.strip())\n",
    "#         lunch_price[i] = (soup_tmp[1].text.strip())\n",
    "\n",
    "#         #print('original', soup_tmp)\n",
    "#         soup_tmp_area = soup_tmp_area.get_text()\n",
    "#         #print('text', soup_tmp)\n",
    "#         if (type(soup_tmp_area) is list):\n",
    "#             areas[i]  = soup_tmp_area.strip().split(' / ')[0].replace('\\n', '')\n",
    "#             genres[i] = soup_tmp_area.strip().split(' / ')[1].replace('\\n', '')\n",
    "#         else:\n",
    "#             areas[i] = '-'\n",
    "#             genres[i] = soup_tmp_area.strip().split(' / ')[0].replace('\\n', '')\n",
    "\n",
    "#         # Extracting the URL\n",
    "#         urls[i] = soup_list[i].get('data-detail-url')\n",
    "\n",
    "\n",
    "#         # # Getting URL information\n",
    "#         # temp_page = requests.get(urls[i])\n",
    "#         # url_soup = BeautifulSoup(temp_page.text,'html.parser')\n",
    "#         # address[i] = url_soup.find(class_=\"rstinfo-table__address\").text.strip()\n",
    "\n",
    "#         temp_data = {\n",
    "#         'Name': names,\n",
    "#         'Rating': ratings,\n",
    "#         'Lunch Price': lunch_price,\n",
    "#         'Dinner Price': dinner_price,\n",
    "#         'Station': areas,\n",
    "#         'Genre': genres,\n",
    "#         'Link': urls,\n",
    "#         'Address': address\n",
    "#         }\n",
    "\n",
    "#         temp_df = pd.DataFrame(temp_data)\n",
    "        \n",
    "#     # Increase counter for pages\n",
    "#     page_count += 1\n",
    "#     df = pd.concat([df, temp_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
